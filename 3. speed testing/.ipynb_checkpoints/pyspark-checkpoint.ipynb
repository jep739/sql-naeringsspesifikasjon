{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fe74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/skatt-naering/src/settings.py\n",
    "%run ~/skatt-naering/production/naeringsspesifikasjon/config_naeringsspesifikasjon.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3049d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyspark.pandas as ps\n",
    "from dapla.auth import AuthClient\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f29d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nst import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e35d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.use_virtualenv_in_pyspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b9cde",
   "metadata": {},
   "source": [
    "# With Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2183ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"ParquetPerformanceTest\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.AbstractFileSystem.gs.impl\",\n",
    "        \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define the paths\n",
    "bredt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt\"\n",
    "langt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt\"\n",
    "# hovedtema_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_partitioned_data/hovedtema\"\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "# total_time_hovedtema = 0\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    bredt_df = spark.read.parquet(bredt_path)\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "    langt_df = spark.read.parquet(langt_path)\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "    # # 'hovedtema' partitioned dataset\n",
    "    # start_time = time.time()\n",
    "    # hovedtema_df = spark.read.parquet(hovedtema_path)\n",
    "    # total_time_hovedtema += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "# avg_time_hovedtema = total_time_hovedtema / iterations\n",
    "\n",
    "print(f\"Average time taken to read 'bredt' dataset: {avg_time_bredt:.2f} seconds\")\n",
    "print(f\"Average time taken to read 'langt' dataset: {avg_time_langt:.2f} seconds\")\n",
    "# print(f\"Average time taken to read 'hovedtema' partitioned dataset: {avg_time_hovedtema:.2f} seconds\")\n",
    "\n",
    "# Remember to stop the SparkSession after your tests\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce303f3",
   "metadata": {},
   "source": [
    "# Without configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the paths\n",
    "bredt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt\"\n",
    "langt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt\"\n",
    "# hovedtema_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_partitioned_data/hovedtema\"\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "# total_time_hovedtema = 0\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    bredt_df = spark.read.parquet(bredt_path)\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "    langt_df = spark.read.parquet(langt_path)\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "    # # 'hovedtema' partitioned dataset\n",
    "    # start_time = time.time()\n",
    "    # hovedtema_df = spark.read.parquet(hovedtema_path)\n",
    "    # total_time_hovedtema += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "# avg_time_hovedtema = total_time_hovedtema / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to read 'bredt' dataset using PySpark: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to read 'langt' dataset using PySpark: {avg_time_langt:.2f} seconds\"\n",
    ")\n",
    "# print(f\"Average time taken to read 'hovedtema' partitioned dataset: {avg_time_hovedtema:.2f} seconds\")\n",
    "\n",
    "# Remember to stop the SparkSession after your tests\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88b65f",
   "metadata": {},
   "source": [
    "# Read in and convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a804e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.use_virtualenv_in_pyspark()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the paths\n",
    "bredt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt\"\n",
    "langt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt\"\n",
    "# hovedtema_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_partitioned_data/hovedtema\"\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "# total_time_hovedtema = 0\n",
    "\n",
    "iterations = 3\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    bredt_df = spark.read.parquet(bredt_path)\n",
    "    bredt_df = bredt_df.toPandas()\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # # 'langt' dataset\n",
    "    # start_time = time.time()\n",
    "    # langt_df = spark.read.parquet(langt_path)\n",
    "    # langt_df = langt_df.toPandas()\n",
    "    # total_time_langt += time.time() - start_time\n",
    "\n",
    "    # # 'hovedtema' partitioned dataset\n",
    "    # start_time = time.time()\n",
    "    # hovedtema_df = spark.read.parquet(hovedtema_path)\n",
    "    # total_time_hovedtema += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "# avg_time_langt = total_time_langt / iterations\n",
    "# avg_time_hovedtema = total_time_hovedtema / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to read 'bredt' dataset using PySpark: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "# print(f\"Average time taken to read 'langt' dataset using PySpark: {avg_time_langt:.2f} seconds\")\n",
    "print(\n",
    "    f\"Average time taken to read 'langt' dataset using PySpark: Failure. Not enough memory\"\n",
    ")\n",
    "# print(f\"Average time taken to read 'hovedtema' partitioned dataset: {avg_time_hovedtema:.2f} seconds\")\n",
    "\n",
    "# Remember to stop the SparkSession after your tests\n",
    "spark.stop()\n",
    "\n",
    "import time\n",
    "\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"PerformanceTesting\").getOrCreate()\n",
    "\n",
    "# Define the paths\n",
    "# Note: Assuming TEMP_PATH has been defined earlier in the code\n",
    "bredt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt\"\n",
    "langt_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt\"\n",
    "partitioned_path = f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/partitioned_langt_data/hovedtema=resultatregnskap/undertema=driftsinntekt\"\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "total_time_partitioned = 0\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "\n",
    "    bredt_df = spark.read.parquet(bredt_path)\n",
    "    filtered_bredt_df = bredt_df.filter(F.col(\"norskIdentifikator\") == \"00002047889\")\n",
    "    aggregation_columns = [\n",
    "        F.sum(F.col(c)).alias(c)\n",
    "        for c in filtered_bredt_df.columns\n",
    "        if c.startswith(\"p3\")\n",
    "    ]\n",
    "    avg_p3_columns = filtered_bredt_df.select(\n",
    "        [F.col(c) for c in filtered_bredt_df.columns if c.startswith(\"p3\")]\n",
    "    ).agg(*aggregation_columns)\n",
    "\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "\n",
    "    langt_df = spark.read.parquet(langt_path)\n",
    "    langt_filtered_df = langt_df.filter(\n",
    "        (F.col(\"norskIdentifikator\") == \"00002047889\")\n",
    "        & F.col(\"felt_id\").startswith(\"p3\")\n",
    "    )\n",
    "    langt_filtered_df = langt_filtered_df.withColumn(\n",
    "        \"felt_verdi\", F.col(\"felt_verdi\").cast(\"double\")\n",
    "    )\n",
    "    avg_p3_value = langt_filtered_df.agg(F.sum(\"felt_verdi\").alias(\"total\")).collect()[\n",
    "        0\n",
    "    ][\"total\"]\n",
    "\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "    # 'partitioned' dataset\n",
    "    start_time = time.time()\n",
    "\n",
    "    partitioned_df = spark.read.parquet(partitioned_path)\n",
    "    partitioned_filtered_df = partitioned_df.filter(\n",
    "        (F.col(\"norskIdentifikator\") == \"00002047889\")\n",
    "        & F.col(\"felt_id\").startswith(\"p3\")\n",
    "    )\n",
    "    partitioned_filtered_df = partitioned_filtered_df.withColumn(\n",
    "        \"felt_verdi\", F.col(\"felt_verdi\").cast(\"double\")\n",
    "    )\n",
    "    avg_p3_partitioned_value = partitioned_filtered_df.agg(\n",
    "        F.sum(\"felt_verdi\").alias(\"total\")\n",
    "    ).collect()[0][\"total\"]\n",
    "\n",
    "    total_time_partitioned += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "avg_time_partitioned = total_time_partitioned / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken using PySpark for the 'bredt' dataset: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken using PySpark for the 'langt' dataset: {avg_time_langt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken using PySpark for the 'partitioned' dataset: {avg_time_partitioned:.2f} seconds\"\n",
    ")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noku-ml",
   "language": "python",
   "name": "noku-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
