{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307abc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/skatt-naering/src/settings.py\n",
    "%run ~/skatt-naering/production/naeringsspesifikasjon/config_naeringsspesifikasjon.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1959ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyspark.pandas as ps\n",
    "from dapla import FileClient\n",
    "from dapla.auth import AuthClient\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c930b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from nst import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f377d",
   "metadata": {},
   "source": [
    "# Read in Data using pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "files1 = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "files2 = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    #     # Filter out only the parquet files\n",
    "\n",
    "    tabley = pq.read_table(files1, filesystem=fs)\n",
    "    # df = tabley.to_pandas()\n",
    "\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    #     # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "    #     # Filter out only the parquet files\n",
    "\n",
    "    tabley = pq.read_table(files2, filesystem=fs)\n",
    "    # df = tabley.to_pandas()\n",
    "\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to read 'bredt' dataset using pyarrow: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to read 'langt' dataset using pyarrow: {avg_time_langt:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbdd52",
   "metadata": {},
   "source": [
    "# Read in data using Pyarrow with DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapla import FileClient\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "# Define the paths\n",
    "bredt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "langt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    bredt_df = dd.read_parquet(bredt_path, engine=\"pyarrow\", filesystem=fs)\n",
    "    # Trigger computation with .compute() if you want to bring data into memory\n",
    "    # bredt_df.compute()\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "    langt_df = dd.read_parquet(langt_path, engine=\"pyarrow\", filesystem=fs)\n",
    "    # Trigger computation with .compute() if you want to bring data into memory\n",
    "    # langt_df.compute()\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to read 'bredt' dataset using Dask with pyarrow engine: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to read 'langt' dataset using Dask with pyarrow engine: {avg_time_langt:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61e2a6",
   "metadata": {},
   "source": [
    "# Read in / create dataframe for data using Pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae026cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapla import FileClient\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "# Define the paths\n",
    "files1 = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "files2 = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "\n",
    "iterations = 3\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    tabley = pq.read_table(files1, filesystem=fs)\n",
    "    df_bredt = tabley.to_pandas()  # Create a Pandas DataFrame\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # # 'langt' dataset\n",
    "    # start_time = time.time()\n",
    "    # tabley = pq.read_table(files2, filesystem=fs)\n",
    "    # df_langt = tabley.to_pandas()  # Create a Pandas DataFrame\n",
    "    # total_time_langt += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "# avg_time_langt = total_time_langt / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to read and create DataFrame for 'bredt' dataset using pyarrow: {avg_time_bredt:.2f} seconds\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5c5ba",
   "metadata": {},
   "source": [
    "# Read in / create dataframe for data using DASK with pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b77501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapla import FileClient\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "# Define the paths\n",
    "bredt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "langt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "\n",
    "iterations = 2\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    bredt_ddf = dd.read_parquet(bredt_path, engine=\"pyarrow\", filesystem=fs)\n",
    "    bredt_ddf = (\n",
    "        bredt_ddf.compute()\n",
    "    )  # Bring data into memory and create a Dask DataFrame\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "    langt_ddf = dd.read_parquet(langt_path, engine=\"pyarrow\", filesystem=fs)\n",
    "    langt_ddf = (\n",
    "        langt_ddf.compute()\n",
    "    )  # Bring data into memory and create a Dask DataFrame\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to read and create DataFrame for 'bredt' dataset using Dask with pyarrow engine: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to read and create DataFrame for 'langt' dataset using Dask with pyarrow engine: {avg_time_langt:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515177c",
   "metadata": {},
   "source": [
    "# Pyarrow with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047dccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapla import FileClient\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "# Define the paths\n",
    "files1 = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "files2 = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "iterations = 3\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "    tabley = pq.read_table(files1, filesystem=fs)\n",
    "    df_bredt = con.execute(\"Select * from tabley\").df()\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "    tabley = pq.read_table(files2, filesystem=fs)\n",
    "    df_langt = con.execute(\"Select * from tabley\").df()\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to read and create DataFrame for 'bredt' dataset using pyarrow: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to read and create DataFrame for 'langt' dataset using pyarrow: {avg_time_langt:.2f} seconds\"\n",
    ")\n",
    "# print(f\"Average time taken to read and create DataFrame for 'langt' dataset using pyarrow: Failure. Not enough memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54d442",
   "metadata": {},
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35009bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e155ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapla import FileClient\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "# Partition the 'langt' dataset\n",
    "partitioned_path = (\n",
    "    f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/partitioned_langt_data\"\n",
    ")\n",
    "langt_df.to_parquet(\n",
    "    partitioned_path,\n",
    "    partition_on=[\"hovedtema\", \"undertema\", \"gruppe\"],\n",
    "    engine=\"pyarrow\",\n",
    "    filesystem=fs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82868859",
   "metadata": {},
   "source": [
    "# Dask queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapla import FileClient\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "# Define the paths\n",
    "bredt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "langt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "partitioned_path_langt = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/partitioned_langt_data/hovedtema=resultatregnskap/undertema=driftsinntekt/**/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "partitioned_path_bredt = [\n",
    "    \"ssb-prod-skatt-naering-data-produkt/temp/naeringsspesifikasjon_data/g2022/resultregnskap_balanseregnskap_testfiler/partitioned_wide_data/resultatregnskap/driftsinntekt/salgsinntekt/salgsinntekt\",\n",
    "    \"ssb-prod-skatt-naering-data-produkt/temp/naeringsspesifikasjon_data/g2022/resultregnskap_balanseregnskap_testfiler/partitioned_wide_data/resultatregnskap/driftsinntekt/annenDriftsinntekt/annenDriftsinntekt\",\n",
    "]\n",
    "\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "total_time_partitioned_langt = 0\n",
    "total_time_partitioned_bredt = 0\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "\n",
    "    bredt_df = dd.read_parquet(\n",
    "        bredt_path,\n",
    "        engine=\"pyarrow\",\n",
    "        filters=[(\"norskIdentifikator\", \"==\", \"00002047889\")],\n",
    "        filesystem=fs,\n",
    "    )\n",
    "\n",
    "    # filtered_bredt_df = bredt_df[bredt_df['NorskIdentifaktor'] == '00002047889']\n",
    "    avg_p3_columns = (\n",
    "        bredt_df[[col for col in bredt_df.columns if col.startswith(\"p3\")]]\n",
    "        .sum()\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # 'langt' dataset\n",
    "    start_time = time.time()\n",
    "\n",
    "    # langt_df = dd.read_parquet(langt_path, engine='pyarrow', filesystem = fs)\n",
    "\n",
    "    # Read only rows with the given norskIdentifikator\n",
    "    langt_df = dd.read_parquet(\n",
    "        langt_path,\n",
    "        engine=\"pyarrow\",\n",
    "        filters=[(\"norskIdentifikator\", \"==\", \"00002047889\")],\n",
    "        filesystem=fs,\n",
    "    )\n",
    "\n",
    "    # Now filter the rows based on the 'felt_id' column\n",
    "    langt_df = langt_df[langt_df[\"felt_id\"].str.startswith(\"p3\")]\n",
    "\n",
    "    # Convert the 'felt_verdi' column to numeric\n",
    "    langt_df[\"felt_verdi\"] = langt_df[\"felt_verdi\"].map_partitions(\n",
    "        pd.to_numeric, errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # query\n",
    "\n",
    "    avg_p3_value = langt_df[\"felt_verdi\"].sum().compute()\n",
    "\n",
    "    total_time_langt += time.time() - start_time\n",
    "\n",
    "    # 'partitioned' dataset\n",
    "    start_time = time.time()\n",
    "\n",
    "    partitioned_df = dd.read_parquet(\n",
    "        partitioned_path_langt,\n",
    "        engine=\"pyarrow\",\n",
    "        filters=[(\"norskIdentifikator\", \"==\", \"00002047889\")],\n",
    "        filesystem=fs,\n",
    "    )\n",
    "\n",
    "    # Now filter the rows based on the 'felt_id' column\n",
    "    partitioned_df = partitioned_df[partitioned_df[\"felt_id\"].str.startswith(\"p3\")]\n",
    "\n",
    "    # Convert the 'felt_verdi' column to numeric\n",
    "    partitioned_df[\"felt_verdi\"] = partitioned_df[\"felt_verdi\"].map_partitions(\n",
    "        pd.to_numeric, errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    avg_p3_value = partitioned_df[\"felt_verdi\"].sum().compute()\n",
    "\n",
    "    total_time_partitioned_langt += time.time() - start_time\n",
    "\n",
    "    # 'partitioned' dataset - wide\n",
    "    start_time = time.time()\n",
    "\n",
    "    partitioned_df = dd.read_parquet(\n",
    "        partitioned_path_bredt,\n",
    "        engine=\"pyarrow\",\n",
    "        filters=[(\"norskIdentifikator\", \"==\", \"00002047889\")],\n",
    "        filesystem=fs,\n",
    "    )\n",
    "\n",
    "    avg_p3_columns = (\n",
    "        partitioned_df[[col for col in partitioned_df.columns if col.startswith(\"p3\")]]\n",
    "        .sum()\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    total_time_partitioned_bredt += time.time() - start_time\n",
    "\n",
    "\n",
    "# Compute the average time for each dataset\n",
    "avg_time_bredt = total_time_bredt / iterations\n",
    "avg_time_langt = total_time_langt / iterations\n",
    "avg_time_partitioned_langt = total_time_partitioned_langt / iterations\n",
    "avg_time_partitioned_bredt = total_time_partitioned_bredt / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken to compute sum_intekter in the 'bredt' dataset for a single foretak using Dask with pyarrow engine: {avg_time_bredt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to compute sum_intekter in the 'langt' dataset for a single foretak using Dask with pyarrow engine: {avg_time_langt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to compute sum_intekter in the 'partitioned_langt' dataset for a single foretak using Dask with pyarrow engine: {avg_time_partitioned_langt:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Average time taken to compute sum_intekter in the 'partitioned_bredt' dataset for a single foretak using Dask with pyarrow engine: {avg_time_partitioned_bredt:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef65efa",
   "metadata": {},
   "source": [
    "# Pyarrow queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150778f",
   "metadata": {},
   "source": [
    "# DuckDB queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6061d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from dapla import FileClient\n",
    "\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "bredt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "langt_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "partitioned_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/partitioned_langt_data/hovedtema=resultatregnskap/undertema=driftsinntekt/**/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "\n",
    "# We will store total time taken for each dataset in these variables\n",
    "total_time_bredt = 0\n",
    "total_time_langt = 0\n",
    "total_time_partitioned = 0\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "conn = duckdb.connect()\n",
    "\n",
    "# We will store total time taken for DuckDB in this variable\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # 'bredt' dataset\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load the Parquet data into DuckDB\n",
    "    conn.execute(\n",
    "        f\"CREATE OR REPLACE VIEW bredt_data AS SELECT * FROM parquet_scan('{bredt_path[0]}') WHERE norskIdentifikator = '00002047889'\"\n",
    "    )\n",
    "\n",
    "    # Calculate average for columns starting with 'p3'\n",
    "    avg_p3_columns_duckdb = conn.execute(\n",
    "        \"SELECT SUM(column_name) FROM bredt_data WHERE column_name LIKE 'p3%'\"\n",
    "    ).fetchall()[0][0]\n",
    "\n",
    "    total_time_bredt += time.time() - start_time\n",
    "\n",
    "    # Repeat similar steps for 'langt' and 'partitioned' dataset if needed\n",
    "\n",
    "# Compute the average time for DuckDB\n",
    "avg_time_duckdb = total_time_duckdb / iterations\n",
    "\n",
    "print(\n",
    "    f\"Average time taken using DuckDB for the 'bredt' dataset: {avg_time_duckdb:.2f} seconds\"\n",
    ")\n",
    "\n",
    "# Clean up (Optional)\n",
    "conn.execute(\"DROP VIEW IF EXISTS bredt_data\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ea7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# import duckdb\n",
    "# from dapla import FileClient\n",
    "\n",
    "# fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "# bredt_path = [\n",
    "#     f\n",
    "#     for f in fs.glob(\n",
    "#         f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_bredt/*\"\n",
    "#     )\n",
    "#     if f.endswith(\".parquet\")\n",
    "# ]\n",
    "# langt_path = [\n",
    "#     f\n",
    "#     for f in fs.glob(\n",
    "#         f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/resultregnskap_balanseregnskap_testfil_langt/*\"\n",
    "#     )\n",
    "#     if f.endswith(\".parquet\")\n",
    "# ]\n",
    "# partitioned_path = [\n",
    "#     f\n",
    "#     for f in fs.glob(\n",
    "#         f\"{TEMP_PATH}/resultregnskap_balanseregnskap_testfiler/partitioned_langt_data/hovedtema=resultatregnskap/undertema=driftsinntekt/**/*\"\n",
    "#     )\n",
    "#     if f.endswith(\".parquet\")\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Initialize DuckDB connection\n",
    "# conn = duckdb.connect()\n",
    "\n",
    "# # We will store total time taken for DuckDB in this variable\n",
    "# total_time_duckdb = 0\n",
    "\n",
    "# # Define a local path to save the downloaded Parquet file\n",
    "# local_path = \"temp_parquet_file.parquet\"\n",
    "\n",
    "# for _ in range(iterations):\n",
    "#     # Download the file from GCS to the local system\n",
    "#     with fs.open(bredt_path[0], \"rb\") as f_remote, open(local_path, \"wb\") as f_local:\n",
    "#         shutil.copyfileobj(f_remote, f_local)\n",
    "\n",
    "#     # 'bredt' dataset\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Load the locally stored Parquet data into DuckDB\n",
    "#     conn.execute(\n",
    "#         f\"CREATE OR REPLACE VIEW bredt_data AS SELECT * FROM parquet_scan('{local_path}') WHERE norskIdentifikator = '00002047889'\"\n",
    "#     )\n",
    "\n",
    "#     # Calculate average for columns starting with 'p3'\n",
    "#     avg_p3_columns_duckdb = conn.execute(\n",
    "#         \"SELECT SUM(column_name) FROM bredt_data WHERE column_name LIKE 'p3%'\"\n",
    "#     ).fetchall()[0][0]\n",
    "\n",
    "#     total_time_duckdb += time.time() - start_time\n",
    "\n",
    "#     # Optionally remove the local file to free up space\n",
    "#     os.remove(local_path)\n",
    "\n",
    "#     # Repeat similar steps for 'langt' and 'partitioned' dataset if needed\n",
    "\n",
    "# # Compute the average time for DuckDB\n",
    "# avg_time_duckdb = total_time_duckdb / iterations\n",
    "\n",
    "# print(\n",
    "#     f\"Average time taken using DuckDB for the 'bredt' dataset: {avg_time_duckdb:.2f} seconds\"\n",
    "# )\n",
    "\n",
    "# # Clean up (Optional)\n",
    "# conn.execute(\"DROP VIEW IF EXISTS bredt_data\")\n",
    "# conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noku-ml",
   "language": "python",
   "name": "noku-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
